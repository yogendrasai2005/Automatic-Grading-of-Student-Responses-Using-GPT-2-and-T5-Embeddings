{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f141624a-79a1-4465-bbe7-572802cd0ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and hyperparameter tuning for Linear Regression...\n",
      "Cross-validation after tuning for Linear Regression:\n",
      "CV Mean RMSE (after tuning): 0.37487434763210814, CV RMSE Std: 0.11737285408923398\n",
      "CV Mean R² (after tuning): 0.843647007689988, CV R² Std: 0.04694554352039047\n",
      "\n",
      "Test RMSE: 0.5548310434227557\n",
      "Test R²: 0.8790793532803853\n",
      "Test CV Mean RMSE: 1.3441109852712294, Test CV RMSE Std: 0.311815702622124\n",
      "Test CV Mean R²: 0.4619267767116873, Test CV R² Std: 0.11816723570212984\n",
      "\n",
      "Training and hyperparameter tuning for Ridge Regression...\n",
      "Best Ridge Regression model: {'alpha': 10}\n",
      "Cross-validation after tuning for Ridge Regression:\n",
      "CV Mean RMSE (after tuning): 0.3671726827763718, CV RMSE Std: 0.10839536206873733\n",
      "CV Mean R² (after tuning): 0.8468276554043591, CV R² Std: 0.043317639966460826\n",
      "\n",
      "Test RMSE: 0.5538478785808704\n",
      "Test R²: 0.87950751820526\n",
      "Test CV Mean RMSE: 0.7574514133702778, Test CV RMSE Std: 0.16620775857211892\n",
      "Test CV Mean R²: 0.6958754006511854, Test CV R² Std: 0.06690453617856962\n",
      "\n",
      "Training and hyperparameter tuning for Lasso Regression...\n",
      "Best Lasso Regression model: {'alpha': 0.1}\n",
      "Cross-validation after tuning for Lasso Regression:\n",
      "CV Mean RMSE (after tuning): 0.48257275722692194, CV RMSE Std: 0.08031858353674455\n",
      "CV Mean R² (after tuning): 0.7984997174765164, CV R² Std: 0.03172442256078311\n",
      "\n",
      "Test RMSE: 0.6926756029960088\n",
      "Test R²: 0.811531515024015\n",
      "Test CV Mean RMSE: 0.5250959973421117, Test CV RMSE Std: 0.125585658469576\n",
      "Test CV Mean R²: 0.7898816964937246, Test CV R² Std: 0.047526583900399946\n",
      "\n",
      "Training and hyperparameter tuning for KNN...\n",
      "Best KNN model: {'n_neighbors': 5, 'weights': 'uniform'}\n",
      "Cross-validation after tuning for KNN:\n",
      "CV Mean RMSE (after tuning): 0.769117949328413, CV RMSE Std: 0.1430272782999771\n",
      "CV Mean R² (after tuning): 0.6789204109534662, CV R² Std: 0.056323282445198006\n",
      "\n",
      "Test RMSE: 0.8410740404754584\n",
      "Test R²: 0.7221263190425633\n",
      "Test CV Mean RMSE: 1.2111949367088608, Test CV RMSE Std: 0.2773926976741836\n",
      "Test CV Mean R²: 0.50082695240884, Test CV R² Std: 0.16990246242016005\n",
      "\n",
      "Training and hyperparameter tuning for Decision Tree...\n",
      "Best Decision Tree model: {'max_depth': None, 'min_samples_split': 2}\n",
      "Cross-validation after tuning for Decision Tree:\n",
      "CV Mean RMSE (after tuning): 0.9446421839969842, CV RMSE Std: 0.1409912849027732\n",
      "CV Mean R² (after tuning): 0.5991791292597811, CV R² Std: 0.065411054783738\n",
      "\n",
      "Test RMSE: 0.9693021198244754\n",
      "Test R²: 0.6309397415000573\n",
      "Test CV Mean RMSE: 2.0708544303797467, Test CV RMSE Std: 0.2424018491375634\n",
      "Test CV Mean R²: 0.1629479445252245, Test CV R² Std: 0.15202897225491974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel(r\"C:\\Users\\dhanu\\OneDrive\\Desktop\\machine learning\\ML TRAIN DATASETS\\train_t5_embeddings.xlsx\")\n",
    "\n",
    "# Features and target variable\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "y = data['output']     # Target variable\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.95)  # Preserve 95% of the variance\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# List of regression models up to Decision Tree\n",
    "models = [\n",
    "    ('Linear Regression', LinearRegression()),\n",
    "    ('Ridge Regression', Ridge()),\n",
    "    ('Lasso Regression', Lasso()),\n",
    "    ('KNN', KNeighborsRegressor()),\n",
    "    ('Decision Tree', DecisionTreeRegressor())\n",
    "]\n",
    "\n",
    "# Function to calculate and return performance metrics\n",
    "def evaluate_model(model, X, y, cv_folds=5):\n",
    "    # Cross-validation with fewer folds (default 5)\n",
    "    cv_scores_rmse = cross_val_score(model, X, y, cv=cv_folds, scoring='neg_mean_squared_error')\n",
    "    cv_scores_r2 = cross_val_score(model, X, y, cv=cv_folds, scoring='r2')\n",
    "\n",
    "    # Compute mean and standard deviation of CV scores\n",
    "    rmse_mean = -cv_scores_rmse.mean()  # Convert negative RMSE to positive\n",
    "    rmse_std = cv_scores_rmse.std()\n",
    "    r2_mean = cv_scores_r2.mean()\n",
    "    r2_std = cv_scores_r2.std()\n",
    "\n",
    "    return rmse_mean, rmse_std, r2_mean, r2_std\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV for the selected models (reduced hyperparameter space)\n",
    "param_grids = {\n",
    "    'Linear Regression': {},  # No hyperparameters for linear regression\n",
    "    'Ridge Regression': {'alpha': [1, 10]},  # Reduced hyperparameter space\n",
    "    'Lasso Regression': {'alpha': [0.1, 1]},  # Reduced hyperparameter space\n",
    "    'KNN': {'n_neighbors': [5], 'weights': ['uniform']},  # Reduced search space\n",
    "    'Decision Tree': {'max_depth': [None, 5], 'min_samples_split': [2, 5]}  # Reduced search space\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning and evaluation for each model\n",
    "for name, model in models:\n",
    "    print(f\"Training and hyperparameter tuning for {name}...\")\n",
    "    \n",
    "    param_grid = param_grids.get(name, {})\n",
    "    \n",
    "    # Step 2: Hyperparameter tuning using GridSearchCV\n",
    "    if param_grid:\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        print(f\"Best {name} model: {grid_search.best_params_}\")\n",
    "    else:\n",
    "        best_model = model.fit(X_train, y_train)\n",
    "\n",
    "    # Step 3: Cross-validation after tuning\n",
    "    print(f\"Cross-validation after tuning for {name}:\")\n",
    "    rmse_mean, rmse_std, r2_mean, r2_std = evaluate_model(best_model, X_train, y_train, cv_folds=5)\n",
    "    print(f\"CV Mean RMSE (after tuning): {rmse_mean}, CV RMSE Std: {rmse_std}\")\n",
    "    print(f\"CV Mean R² (after tuning): {r2_mean}, CV R² Std: {r2_std}\\n\")\n",
    "\n",
    "    # Step 4: Testing on the test set with the best model\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    # Test CV scores\n",
    "    test_cv_rmse_mean, test_cv_rmse_std, test_cv_r2_mean, test_cv_r2_std = evaluate_model(best_model, X_test, y_test, cv_folds=5)\n",
    "\n",
    "    print(f\"Test RMSE: {test_rmse}\")\n",
    "    print(f\"Test R²: {test_r2}\")\n",
    "    print(f\"Test CV Mean RMSE: {test_cv_rmse_mean}, Test CV RMSE Std: {test_cv_rmse_std}\")\n",
    "    print(f\"Test CV Mean R²: {test_cv_r2_mean}, Test CV R² Std: {test_cv_r2_std}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db8a8a15-3ddc-4a53-818f-ae4ceae4ae9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and hyperparameter tuning for XGBoost...\n",
      "Best XGBoost model: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 150, 'subsample': 0.7}\n",
      "Cross-validation after tuning for XGBoost:\n",
      "CV Mean RMSE (after tuning): 0.30264910238362186, CV RMSE Std: 0.07346926240677541\n",
      "CV Mean R² (after tuning): 0.8739823341369629, CV R² Std: 0.02821056692247991\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhanu\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.5069311578811253\n",
      "Test R²: 0.8990568518638611\n",
      "Test CV Mean RMSE: 0.8180029381265006, Test CV RMSE Std: 0.11758197709668125\n",
      "Test CV Mean R²: 0.675088107585907, Test CV R² Std: 0.02447463625002074\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel(r\"C:\\Users\\dhanu\\OneDrive\\Desktop\\machine learning\\ML TRAIN DATASETS\\train_t5_embeddings.xlsx\")\n",
    "\n",
    "# Features and target variable\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "y = data['output']     # Target variable\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.95)  # Preserve 95% of the variance\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# XGBoost model\n",
    "model = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Reduced hyperparameter tuning grid (fewer combinations)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150],               # Fewer estimators\n",
    "    'learning_rate': [0.01, 0.1],             # Limited learning rates\n",
    "    'max_depth': [3, 5],                      # Shallower trees\n",
    "    'subsample': [0.7, 0.8],                  # Slightly lower subsample values\n",
    "    'colsample_bytree': [0.8, 1.0]            # Adjusted column sampling\n",
    "}\n",
    "\n",
    "# Hyperparameter tuning and evaluation with fewer cross-validation folds\n",
    "print(f\"Training and hyperparameter tuning for XGBoost...\")\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)  # Reduced CV folds\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best XGBoost model: {grid_search.best_params_}\")\n",
    "\n",
    "# Function to calculate and return performance metrics\n",
    "def evaluate_model(model, X, y, cv_folds=5):\n",
    "    # Cross-validation with fewer folds (default 5)\n",
    "    cv_scores_rmse = cross_val_score(model, X, y, cv=cv_folds, scoring='neg_mean_squared_error')  # Reduced folds\n",
    "    cv_scores_r2 = cross_val_score(model, X, y, cv=cv_folds, scoring='r2')  # Reduced folds\n",
    "\n",
    "    # Compute mean and standard deviation of CV scores\n",
    "    rmse_mean = -cv_scores_rmse.mean()  # Convert negative RMSE to positive\n",
    "    rmse_std = cv_scores_rmse.std()\n",
    "    r2_mean = cv_scores_r2.mean()\n",
    "    r2_std = cv_scores_r2.std()\n",
    "\n",
    "    return rmse_mean, rmse_std, r2_mean, r2_std\n",
    "\n",
    "# Step 1: Cross-validation after tuning (on training set)\n",
    "print(f\"Cross-validation after tuning for XGBoost:\")\n",
    "train_rmse_mean, train_rmse_std, train_r2_mean, train_r2_std = evaluate_model(best_model, X_train, y_train, cv_folds=5)\n",
    "print(f\"CV Mean RMSE (after tuning): {train_rmse_mean}, CV RMSE Std: {train_rmse_std}\")\n",
    "print(f\"CV Mean R² (after tuning): {train_r2_mean}, CV R² Std: {train_r2_std}\\n\")\n",
    "\n",
    "# Step 2: Testing on the test set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)  # RMSE\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Test CV scores\n",
    "test_cv_rmse_mean, test_cv_rmse_std, test_cv_r2_mean, test_cv_r2_std = evaluate_model(best_model, X_test, y_test, cv_folds=5)\n",
    "\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test R²: {test_r2}\")\n",
    "print(f\"Test CV Mean RMSE: {test_cv_rmse_mean}, Test CV RMSE Std: {test_cv_rmse_std}\")\n",
    "print(f\"Test CV Mean R²: {test_cv_r2_mean}, Test CV R² Std: {test_cv_r2_std}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7b0fb89-d5c4-4dcb-bca5-1bbf99c093f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and hyperparameter tuning for AdaBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhanu\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AdaBoost model: {'learning_rate': 0.1, 'loss': 'square', 'n_estimators': 100}\n",
      "Cross-validation after tuning for AdaBoost:\n",
      "CV Mean RMSE (after tuning): 0.9332836827304088, CV RMSE Std: 0.06543115382323982\n",
      "CV Mean R² (after tuning): 0.6101715510278675, CV R² Std: 0.02223883896145461\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhanu\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 1.015017723202473\n",
      "Test R²: 0.5953065199531684\n",
      "Test CV Mean RMSE: 0.930766641580392, Test CV RMSE Std: 0.11734322898608636\n",
      "Test CV Mean R²: 0.6291314952795777, Test CV R² Std: 0.029968676539522374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel(r\"C:\\Users\\dhanu\\OneDrive\\Desktop\\machine learning\\ML TRAIN DATASETS\\train_t5_embeddings.xlsx\")\n",
    "\n",
    "# Features and target variable\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "y = data['output']     # Target variable\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.95)  # Preserve 95% of variance\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# AdaBoost model\n",
    "model = AdaBoostRegressor(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],  # Reduced options for faster execution\n",
    "    'learning_rate': [0.01, 0.1],  # Reduced learning rate options\n",
    "    'loss': ['linear', 'square']  # Reduced loss functions for faster testing\n",
    "}\n",
    "\n",
    "# Hyperparameter tuning and evaluation\n",
    "print(f\"Training and hyperparameter tuning for AdaBoost...\")\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)  # Reduced cv folds to speed up\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best AdaBoost model: {grid_search.best_params_}\")\n",
    "\n",
    "# Function to calculate and return performance metrics\n",
    "def evaluate_model(model, X, y, cv_folds=5):\n",
    "    # Cross-validation with 5 folds (default 5)\n",
    "    cv_scores_rmse = cross_val_score(model, X, y, cv=cv_folds, scoring='neg_mean_squared_error')  # Reduced folds\n",
    "    cv_scores_r2 = cross_val_score(model, X, y, cv=cv_folds, scoring='r2')  # Reduced folds\n",
    "\n",
    "    # Compute mean and standard deviation of CV scores\n",
    "    rmse_mean = -cv_scores_rmse.mean()  # Convert negative RMSE to positive\n",
    "    rmse_std = cv_scores_rmse.std()\n",
    "    r2_mean = cv_scores_r2.mean()\n",
    "    r2_std = cv_scores_r2.std()\n",
    "\n",
    "    return rmse_mean, rmse_std, r2_mean, r2_std\n",
    "\n",
    "# Step 1: Cross-validation after tuning (on training set)\n",
    "print(f\"Cross-validation after tuning for AdaBoost:\")\n",
    "train_rmse_mean, train_rmse_std, train_r2_mean, train_r2_std = evaluate_model(best_model, X_train, y_train, cv_folds=5)\n",
    "print(f\"CV Mean RMSE (after tuning): {train_rmse_mean}, CV RMSE Std: {train_rmse_std}\")\n",
    "print(f\"CV Mean R² (after tuning): {train_r2_mean}, CV R² Std: {train_r2_std}\\n\")\n",
    "\n",
    "# Step 2: Testing on the test set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)  # RMSE\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Test CV scores\n",
    "test_cv_rmse_mean, test_cv_rmse_std, test_cv_r2_mean, test_cv_r2_std = evaluate_model(best_model, X_test, y_test, cv_folds=5)\n",
    "\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test R²: {test_r2}\")\n",
    "print(f\"Test CV Mean RMSE: {test_cv_rmse_mean}, Test CV RMSE Std: {test_cv_rmse_std}\")\n",
    "print(f\"Test CV Mean R²: {test_cv_r2_mean}, Test CV R² Std: {test_cv_r2_std}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0f7f1ee-47c9-4797-949e-7a4f27a078df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and hyperparameter tuning for Gradient Boosting...\n",
      "Best Gradient Boosting model: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Cross-validation after tuning for Gradient Boosting:\n",
      "CV Mean RMSE (after tuning): 0.31425095011185067, CV RMSE Std: 0.0774208829864673\n",
      "CV Mean R² (after tuning): 0.8691863319167373, CV R² Std: 0.029663850790337676\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhanu\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.5073878232713636\n",
      "Test R²: 0.8988748829208911\n",
      "Test CV Mean RMSE: 0.951590126592565, Test CV RMSE Std: 0.17648237777062908\n",
      "Test CV Mean R²: 0.6187794594053193, Test CV R² Std: 0.06942117873365039\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel(r\"C:\\Users\\dhanu\\OneDrive\\Desktop\\machine learning\\ML TRAIN DATASETS\\train_t5_embeddings.xlsx\")\n",
    "\n",
    "# Features and target variable\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "y = data['output']     # Target variable\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.95)  # Preserve 95% of variance\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Gradient Boosting model\n",
    "model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],  # Reduced options for faster execution\n",
    "    'learning_rate': [0.01, 0.1],  # Reduced learning rate options\n",
    "    'max_depth': [3, 5],  # Reduced depth for faster training\n",
    "    'subsample': [0.7, 0.8]  # Reduced subsample options\n",
    "}\n",
    "\n",
    "# Hyperparameter tuning and evaluation\n",
    "print(f\"Training and hyperparameter tuning for Gradient Boosting...\")\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)  # Reduced cv folds to speed up\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best Gradient Boosting model: {grid_search.best_params_}\")\n",
    "\n",
    "# Function to calculate and return performance metrics\n",
    "def evaluate_model(model, X, y, cv_folds=5):\n",
    "    # Cross-validation with 5 folds (default 5)\n",
    "    cv_scores_rmse = cross_val_score(model, X, y, cv=cv_folds, scoring='neg_mean_squared_error')  # Reduced folds\n",
    "    cv_scores_r2 = cross_val_score(model, X, y, cv=cv_folds, scoring='r2')  # Reduced folds\n",
    "\n",
    "    # Compute mean and standard deviation of CV scores\n",
    "    rmse_mean = -cv_scores_rmse.mean()  # Convert negative RMSE to positive\n",
    "    rmse_std = cv_scores_rmse.std()\n",
    "    r2_mean = cv_scores_r2.mean()\n",
    "    r2_std = cv_scores_r2.std()\n",
    "\n",
    "    return rmse_mean, rmse_std, r2_mean, r2_std\n",
    "\n",
    "# Step 1: Cross-validation after tuning (on training set)\n",
    "print(f\"Cross-validation after tuning for Gradient Boosting:\")\n",
    "train_rmse_mean, train_rmse_std, train_r2_mean, train_r2_std = evaluate_model(best_model, X_train, y_train, cv_folds=5)\n",
    "print(f\"CV Mean RMSE (after tuning): {train_rmse_mean}, CV RMSE Std: {train_rmse_std}\")\n",
    "print(f\"CV Mean R² (after tuning): {train_r2_mean}, CV R² Std: {train_r2_std}\\n\")\n",
    "\n",
    "# Step 2: Testing on the test set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)  # RMSE\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Test CV scores\n",
    "test_cv_rmse_mean, test_cv_rmse_std, test_cv_r2_mean, test_cv_r2_std = evaluate_model(best_model, X_test, y_test, cv_folds=5)\n",
    "\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test R²: {test_r2}\")\n",
    "print(f\"Test CV Mean RMSE: {test_cv_rmse_mean}, Test CV RMSE Std: {test_cv_rmse_std}\")\n",
    "print(f\"Test CV Mean R²: {test_cv_r2_mean}, Test CV R² Std: {test_cv_r2_std}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d84dbae6-85cb-4a71-a13f-5f27bc364e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and hyperparameter tuning for Random Forest...\n",
      "Best Random Forest model: {'bootstrap': True, 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Cross-validation after tuning for Random Forest:\n",
      "CV Mean RMSE (after tuning): 0.43973313810891695, CV RMSE Std: 0.08149588294507212\n",
      "CV Mean R² (after tuning): 0.8167652011797897, CV R² Std: 0.03036305273527818\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhanu\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.6677125071949344\n",
      "Test R²: 0.8248710357921665\n",
      "Test CV Mean RMSE: 1.0298020901898732, Test CV RMSE Std: 0.15767461895020266\n",
      "Test CV Mean R²: 0.5909487222130052, Test CV R² Std: 0.03743159712114883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel(r\"C:\\Users\\dhanu\\OneDrive\\Desktop\\machine learning\\ML TRAIN DATASETS\\train_t5_embeddings.xlsx\")\n",
    "\n",
    "# Features and target variable\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "y = data['output']     # Target variable\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.95)  # Preserve 95% of variance\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Random Forest model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],  # Reduced estimators for faster execution\n",
    "    'max_depth': [None, 10, 20],  # Reduced depth options\n",
    "    'min_samples_split': [2, 5],  # Limited to lower values for faster computation\n",
    "    'min_samples_leaf': [1, 2],   # Limited leaf size options\n",
    "    'bootstrap': [True]           # No need to test False for bootstrap\n",
    "}\n",
    "\n",
    "# Hyperparameter tuning and evaluation\n",
    "print(f\"Training and hyperparameter tuning for Random Forest...\")\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)  # Reduced cv folds to speed up\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best Random Forest model: {grid_search.best_params_}\")\n",
    "\n",
    "# Function to calculate and return performance metrics\n",
    "def evaluate_model(model, X, y, cv_folds=5):\n",
    "    # Cross-validation with 5 folds (default 5)\n",
    "    cv_scores_rmse = cross_val_score(model, X, y, cv=cv_folds, scoring='neg_mean_squared_error')  # Reduced folds\n",
    "    cv_scores_r2 = cross_val_score(model, X, y, cv=cv_folds, scoring='r2')  # Reduced folds\n",
    "\n",
    "    # Compute mean and standard deviation of CV scores\n",
    "    rmse_mean = -cv_scores_rmse.mean()  # Convert negative RMSE to positive\n",
    "    rmse_std = cv_scores_rmse.std()\n",
    "    r2_mean = cv_scores_r2.mean()\n",
    "    r2_std = cv_scores_r2.std()\n",
    "\n",
    "    return rmse_mean, rmse_std, r2_mean, r2_std\n",
    "\n",
    "# Step 1: Cross-validation after tuning (on training set)\n",
    "print(f\"Cross-validation after tuning for Random Forest:\")\n",
    "train_rmse_mean, train_rmse_std, train_r2_mean, train_r2_std = evaluate_model(best_model, X_train, y_train, cv_folds=5)\n",
    "print(f\"CV Mean RMSE (after tuning): {train_rmse_mean}, CV RMSE Std: {train_rmse_std}\")\n",
    "print(f\"CV Mean R² (after tuning): {train_r2_mean}, CV R² Std: {train_r2_std}\\n\")\n",
    "\n",
    "# Step 2: Testing on the test set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)  # RMSE\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Test CV scores\n",
    "test_cv_rmse_mean, test_cv_rmse_std, test_cv_r2_mean, test_cv_r2_std = evaluate_model(best_model, X_test, y_test, cv_folds=5)\n",
    "\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test R²: {test_r2}\")\n",
    "print(f\"Test CV Mean RMSE: {test_cv_rmse_mean}, Test CV RMSE Std: {test_cv_rmse_std}\")\n",
    "print(f\"Test CV Mean R²: {test_cv_r2_mean}, Test CV R² Std: {test_cv_r2_std}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "411a9cf5-2dc4-47b4-a960-9ae1a91244ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components selected by PCA: 214\n",
      "Training and hyperparameter tuning for SVR...\n",
      "Best SVR model: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Cross-validation after tuning for SVR:\n",
      "CV Mean RMSE (after tuning): 0.16413393490872874, CV RMSE Std: 0.03617572079374697\n",
      "CV Mean R² (after tuning): 0.9314636635239119, CV R² Std: 0.014444859261490901\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhanu\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.3664838170652731\n",
      "Test R²: 0.9472419711762071\n",
      "Test CV Mean RMSE: 0.4009073041761133, Test CV RMSE Std: 0.07830549772424744\n",
      "Test CV Mean R²: 0.8392390758780021, Test CV R² Std: 0.03144014144900435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel(r\"C:\\Users\\dhanu\\OneDrive\\Desktop\\machine learning\\ML TRAIN DATASETS\\train_t5_embeddings.xlsx\")\n",
    "\n",
    "# Features and target variable\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "y = data['output']     # Target variable\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "print(f\"Number of components selected by PCA: {pca.n_components_}\")\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# SVR model\n",
    "model = SVR()\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  # Reduced values for faster training\n",
    "    'kernel': ['linear', 'rbf'],  # Focus on common kernels\n",
    "    'gamma': ['scale'],  # Common gamma values\n",
    "}\n",
    "\n",
    "# Hyperparameter tuning and evaluation\n",
    "print(f\"Training and hyperparameter tuning for SVR...\")\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)  # Reduced cv folds to speed up\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best SVR model: {grid_search.best_params_}\")\n",
    "\n",
    "# Function to calculate and return performance metrics\n",
    "def evaluate_model(model, X, y, cv_folds=5):\n",
    "    # Cross-validation with 5 folds (default 5)\n",
    "    cv_scores_rmse = cross_val_score(model, X, y, cv=cv_folds, scoring='neg_mean_squared_error')  # Reduced folds\n",
    "    cv_scores_r2 = cross_val_score(model, X, y, cv=cv_folds, scoring='r2')  # Reduced folds\n",
    "\n",
    "    # Compute mean and standard deviation of CV scores\n",
    "    rmse_mean = -cv_scores_rmse.mean()  # Convert negative RMSE to positive\n",
    "    rmse_std = cv_scores_rmse.std()\n",
    "    r2_mean = cv_scores_r2.mean()\n",
    "    r2_std = cv_scores_r2.std()\n",
    "\n",
    "    return rmse_mean, rmse_std, r2_mean, r2_std\n",
    "\n",
    "# Step 1: Cross-validation after tuning (on training set)\n",
    "print(f\"Cross-validation after tuning for SVR:\")\n",
    "train_rmse_mean, train_rmse_std, train_r2_mean, train_r2_std = evaluate_model(best_model, X_train, y_train, cv_folds=5)\n",
    "print(f\"CV Mean RMSE (after tuning): {train_rmse_mean}, CV RMSE Std: {train_rmse_std}\")\n",
    "print(f\"CV Mean R² (after tuning): {train_r2_mean}, CV R² Std: {train_r2_std}\\n\")\n",
    "\n",
    "# Step 2: Testing on the test set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)  # RMSE\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Test CV scores\n",
    "test_cv_rmse_mean, test_cv_rmse_std, test_cv_r2_mean, test_cv_r2_std = evaluate_model(best_model, X_test, y_test, cv_folds=5)\n",
    "\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test R²: {test_r2}\")\n",
    "print(f\"Test CV Mean RMSE: {test_cv_rmse_mean}, Test CV RMSE Std: {test_cv_rmse_std}\")\n",
    "print(f\"Test CV Mean R²: {test_cv_r2_mean}, Test CV R² Std: {test_cv_r2_std}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104e5741-21a6-4bdc-8741-983612049b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
