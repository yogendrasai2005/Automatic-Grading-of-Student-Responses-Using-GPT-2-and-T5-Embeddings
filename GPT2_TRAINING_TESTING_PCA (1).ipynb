{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d48ccd88-c22c-425c-ac1c-fcc546386ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and hyperparameter tuning for Linear Regression...\n",
      "Cross-validation after tuning for Linear Regression:\n",
      "CV Mean RMSE (after tuning): 0.34735736022123026, CV RMSE Std: 0.0511733615527798\n",
      "CV Mean R² (after tuning): 0.8537205153844616, CV R² Std: 0.02436128878256942\n",
      "\n",
      "Test RMSE: 0.6043519793460793\n",
      "Test R²: 0.8565307413397948\n",
      "Test CV Mean RMSE: 0.6765945963874704, Test CV RMSE Std: 0.2131053283920084\n",
      "Test CV Mean R²: 0.7244934723658905, Test CV R² Std: 0.09209450511537234\n",
      "\n",
      "Training and hyperparameter tuning for Ridge Regression...\n",
      "Best Ridge Regression model: {'alpha': 100}\n",
      "Cross-validation after tuning for Ridge Regression:\n",
      "CV Mean RMSE (after tuning): 0.34208842353388536, CV RMSE Std: 0.05107372275185737\n",
      "CV Mean R² (after tuning): 0.8559935336690273, CV R² Std: 0.023783216423366355\n",
      "\n",
      "Test RMSE: 0.6029482161740294\n",
      "Test R²: 0.8571964559248534\n",
      "Test CV Mean RMSE: 0.5030530961336431, Test CV RMSE Std: 0.18011660990414793\n",
      "Test CV Mean R²: 0.7940899617380046, Test CV R² Std: 0.08053148014517811\n",
      "\n",
      "Training and hyperparameter tuning for Lasso Regression...\n",
      "Best Lasso Regression model: {'alpha': 0.1}\n",
      "Cross-validation after tuning for Lasso Regression:\n",
      "CV Mean RMSE (after tuning): 0.44042065777873274, CV RMSE Std: 0.06475827450454574\n",
      "CV Mean R² (after tuning): 0.8146543130001127, CV R² Std: 0.0294914147433113\n",
      "\n",
      "Test RMSE: 0.676955662774739\n",
      "Test R²: 0.8199888495299563\n",
      "Test CV Mean RMSE: 0.4986576294940697, Test CV RMSE Std: 0.21999608655890837\n",
      "Test CV Mean R²: 0.7955974578019905, Test CV R² Std: 0.09691662264898863\n",
      "\n",
      "Training and hyperparameter tuning for KNN...\n",
      "Best KNN model: {'n_neighbors': 10, 'weights': 'distance'}\n",
      "Cross-validation after tuning for KNN:\n",
      "CV Mean RMSE (after tuning): 0.29859715056904274, CV RMSE Std: 0.05672512741500096\n",
      "CV Mean R² (after tuning): 0.8746196733046062, CV R² Std: 0.02339129143523977\n",
      "\n",
      "Test RMSE: 0.5986855969869909\n",
      "Test R²: 0.859208454332221\n",
      "Test CV Mean RMSE: 0.7107697705264645, Test CV RMSE Std: 0.22027182547220772\n",
      "Test CV Mean R²: 0.7045972511021965, Test CV R² Std: 0.1166428638229519\n",
      "\n",
      "Training and hyperparameter tuning for Decision Tree...\n",
      "Best Decision Tree model: {'max_depth': 10, 'min_samples_split': 2}\n",
      "Cross-validation after tuning for Decision Tree:\n",
      "CV Mean RMSE (after tuning): 0.6629474513019122, CV RMSE Std: 0.11948060089946666\n",
      "CV Mean R² (after tuning): 0.7445254216884818, CV R² Std: 0.0475085762948321\n",
      "\n",
      "Test RMSE: 0.8459271891910154\n",
      "Test R²: 0.7189103048596783\n",
      "Test CV Mean RMSE: 1.8451087521890444, Test CV RMSE Std: 0.49141668467487915\n",
      "Test CV Mean R²: 0.2801124506880398, Test CV R² Std: 0.23423985623409213\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel(r\"C:/Users/dhanu/OneDrive/Desktop/machine learning/ML TRAIN DATASETS/gpt2 embeddings.xlsx\")\n",
    "\n",
    "# Drop any irrelevant columns, such as text or index columns\n",
    "data = data.drop(columns=['Equation', 'GPT2_Embedding'], errors='ignore')\n",
    "\n",
    "# Features and target variable\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "y = data['output']     # Target variable\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.95)  # Preserve 95% of variance\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# List of regression models up to Decision Tree\n",
    "models = [\n",
    "    ('Linear Regression', LinearRegression()),\n",
    "    ('Ridge Regression', Ridge()),\n",
    "    ('Lasso Regression', Lasso()),\n",
    "    ('KNN', KNeighborsRegressor()),\n",
    "    ('Decision Tree', DecisionTreeRegressor())\n",
    "]\n",
    "\n",
    "# Function to calculate and return performance metrics\n",
    "def evaluate_model(model, X, y):\n",
    "    # Cross-validation with 10 folds\n",
    "    cv_scores_rmse = cross_val_score(model, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "    cv_scores_r2 = cross_val_score(model, X, y, cv=10, scoring='r2')\n",
    "\n",
    "    # Compute mean and standard deviation of CV scores\n",
    "    rmse_mean = -cv_scores_rmse.mean()  # Convert negative RMSE to positive\n",
    "    rmse_std = cv_scores_rmse.std()\n",
    "    r2_mean = cv_scores_r2.mean()\n",
    "    r2_std = cv_scores_r2.std()\n",
    "\n",
    "    return rmse_mean, rmse_std, r2_mean, r2_std\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV for the selected models\n",
    "param_grids = {\n",
    "    'Linear Regression': {},  # No hyperparameters for linear regression\n",
    "    'Ridge Regression': {'alpha': [0.1, 1, 10, 100]},\n",
    "    'Lasso Regression': {'alpha': [0.1, 1, 10]},\n",
    "    'KNN': {'n_neighbors': [3, 5, 10, 15], 'weights': ['uniform', 'distance']},\n",
    "    'Decision Tree': {'max_depth': [None, 5, 10, 20], 'min_samples_split': [2, 5, 10]}\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning and evaluation for each model\n",
    "for name, model in models:\n",
    "    print(f\"\\nTraining and hyperparameter tuning for {name}...\")\n",
    "    param_grid = param_grids.get(name, {})\n",
    "\n",
    "    # Skip models with no parameters to tune\n",
    "    if param_grid:\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        print(f\"Best {name} model: {grid_search.best_params_}\")\n",
    "    else:\n",
    "        best_model = model\n",
    "        best_model.fit(X_train, y_train)  # Explicitly fit the model if no hyperparameters are tuned\n",
    "\n",
    "    # Cross-validation after tuning\n",
    "    rmse_mean, rmse_std, r2_mean, r2_std = evaluate_model(best_model, X_train, y_train)\n",
    "    print(f\"Cross-validation after tuning for {name}:\")\n",
    "    print(f\"CV Mean RMSE (after tuning): {rmse_mean}, CV RMSE Std: {rmse_std}\")\n",
    "    print(f\"CV Mean R² (after tuning): {r2_mean}, CV R² Std: {r2_std}\")\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Cross-validation on the test data\n",
    "    test_cv_rmse_mean, test_cv_rmse_std, test_cv_r2_mean, test_cv_r2_std = evaluate_model(best_model, X_test, y_test)\n",
    "\n",
    "    print(f\"\\nTest RMSE: {test_rmse}\")\n",
    "    print(f\"Test R²: {test_r2}\")\n",
    "    print(f\"Test CV Mean RMSE: {test_cv_rmse_mean}, Test CV RMSE Std: {test_cv_rmse_std}\")\n",
    "    print(f\"Test CV Mean R²: {test_cv_r2_mean}, Test CV R² Std: {test_cv_r2_std}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e6f6058-d1f8-4ea8-a86f-4374e20b8ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and hyperparameter tuning for XGBoost...\n",
      "Best XGBoost model: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Cross-validation after tuning for XGBoost:\n",
      "CV Mean RMSE (after tuning): 0.2481026887887942, CV RMSE Std: 0.03686741521643318\n",
      "CV Mean R² (after tuning): 0.8965616226196289, CV R² Std: 0.013336679315897253\n",
      "\n",
      "Test RMSE: 0.5017688404643853\n",
      "Test R²: 0.9011022448539734\n",
      "Test CV Mean RMSE: 0.6818448973939629, Test CV RMSE Std: 0.17073820678551446\n",
      "Test CV Mean R²: 0.7290929317474365, Test CV R² Std: 0.0569218551728663\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel(r\"C:/Users/dhanu/OneDrive/Desktop/machine learning/ML TRAIN DATASETS/gpt2 embeddings.xlsx\")\n",
    "\n",
    "# Drop any irrelevant columns, such as text or index columns\n",
    "data = data.drop(columns=['Equation', 'GPT2_Embedding'], errors='ignore')\n",
    "\n",
    "# Features and target variable\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "y = data['output']     # Target variable\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.95)  # Preserve 95% of variance\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# XGBoost model\n",
    "model = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Reduced hyperparameter tuning grid (fewer combinations)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],               # Fewer estimators\n",
    "    'max_depth': [3, 6],                       # Shallower trees\n",
    "    'learning_rate': [0.01, 0.1],             # Different learning rates\n",
    "    'subsample': [0.8, 1.0]                    # Fraction of samples used for training\n",
    "}\n",
    "\n",
    "# Hyperparameter tuning and evaluation with fewer cross-validation folds\n",
    "print(f\"Training and hyperparameter tuning for XGBoost...\")\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)  # Reduced CV folds\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best XGBoost model: {grid_search.best_params_}\")\n",
    "\n",
    "# Function to calculate and return performance metrics\n",
    "def evaluate_model(model, X, y):\n",
    "    # Cross-validation with fewer folds\n",
    "    cv_scores_rmse = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')  # Reduced folds\n",
    "    cv_scores_r2 = cross_val_score(model, X, y, cv=5, scoring='r2')  # Reduced folds\n",
    "\n",
    "    # Compute mean and standard deviation of CV scores\n",
    "    rmse_mean = -cv_scores_rmse.mean()  # Convert negative RMSE to positive\n",
    "    rmse_std = cv_scores_rmse.std()\n",
    "    r2_mean = cv_scores_r2.mean()\n",
    "    r2_std = cv_scores_r2.std()\n",
    "\n",
    "    return rmse_mean, rmse_std, r2_mean, r2_std\n",
    "\n",
    "# Cross-validation performance\n",
    "rmse_mean, rmse_std, r2_mean, r2_std = evaluate_model(best_model, X_train, y_train)\n",
    "\n",
    "print(f\"Cross-validation after tuning for XGBoost:\")\n",
    "print(f\"CV Mean RMSE (after tuning): {rmse_mean}, CV RMSE Std: {rmse_std}\")\n",
    "print(f\"CV Mean R² (after tuning): {r2_mean}, CV R² Std: {r2_std}\\n\")\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Cross-validation performance on the test set\n",
    "test_cv_rmse_mean, test_cv_rmse_std, test_cv_r2_mean, test_cv_r2_std = evaluate_model(best_model, X_test, y_test)\n",
    "\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test R²: {test_r2}\")\n",
    "print(f\"Test CV Mean RMSE: {test_cv_rmse_mean}, Test CV RMSE Std: {test_cv_rmse_std}\")\n",
    "print(f\"Test CV Mean R²: {test_cv_r2_mean}, Test CV R² Std: {test_cv_r2_std}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c3d339a-70e1-4305-b5bc-9dd35e589f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and hyperparameter tuning for AdaBoost...\n",
      "Best AdaBoost model: {'learning_rate': 0.1, 'loss': 'square', 'n_estimators': 100}\n",
      "Cross-validation after tuning for AdaBoost:\n",
      "CV Mean RMSE (after tuning): 0.7292156034271668, CV RMSE Std: 0.037032436282969136\n",
      "CV Mean R² (after tuning): 0.6951365083185609, CV R² Std: 0.014633281954732093\n",
      "\n",
      "Test RMSE: 0.8923391890529824\n",
      "Test R²: 0.6872200618928292\n",
      "Test CV Mean RMSE: 0.8966253084403478, Test CV RMSE Std: 0.17073283300014006\n",
      "Test CV Mean R²: 0.6431405607693669, Test CV R² Std: 0.05518343747168119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel(r\"C:/Users/dhanu/OneDrive/Desktop/machine learning/ML TRAIN DATASETS/gpt2 embeddings.xlsx\")\n",
    "\n",
    "# Drop any irrelevant columns, such as text or index columns\n",
    "data = data.drop(columns=['Equation', 'GPT2_Embedding'], errors='ignore')\n",
    "\n",
    "# Features and target variable\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "y = data['output']     # Target variable\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.95)  # Preserve 95% of the variance\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# AdaBoost model\n",
    "model = AdaBoostRegressor(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],             # Number of boosting rounds\n",
    "    'learning_rate': [0.01, 0.1],         # Learning rate\n",
    "    'loss': ['linear', 'square']          # Loss function options\n",
    "}\n",
    "\n",
    "print(f\"Training and hyperparameter tuning for AdaBoost...\")\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)  # 5-fold CV\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best AdaBoost model: {grid_search.best_params_}\")\n",
    "\n",
    "# Function to calculate and return performance metrics\n",
    "def evaluate_model(model, X, y):\n",
    "    cv_scores_rmse = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_scores_r2 = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "\n",
    "    rmse_mean = -cv_scores_rmse.mean()   # Convert negative RMSE to positive\n",
    "    rmse_std = cv_scores_rmse.std()\n",
    "    r2_mean = cv_scores_r2.mean()\n",
    "    r2_std = cv_scores_r2.std()\n",
    "\n",
    "    return rmse_mean, rmse_std, r2_mean, r2_std\n",
    "\n",
    "# Cross-validation performance\n",
    "rmse_mean, rmse_std, r2_mean, r2_std = evaluate_model(best_model, X_train, y_train)\n",
    "\n",
    "print(f\"Cross-validation after tuning for AdaBoost:\")\n",
    "print(f\"CV Mean RMSE (after tuning): {rmse_mean}, CV RMSE Std: {rmse_std}\")\n",
    "print(f\"CV Mean R² (after tuning): {r2_mean}, CV R² Std: {r2_std}\\n\")\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Cross-validation performance on the test set\n",
    "test_cv_rmse_mean, test_cv_rmse_std, test_cv_r2_mean, test_cv_r2_std = evaluate_model(best_model, X_test, y_test)\n",
    "\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test R²: {test_r2}\")\n",
    "print(f\"Test CV Mean RMSE: {test_cv_rmse_mean}, Test CV RMSE Std: {test_cv_rmse_std}\")\n",
    "print(f\"Test CV Mean R²: {test_cv_r2_mean}, Test CV R² Std: {test_cv_r2_std}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "722c3b4b-4de4-44e9-8505-a04846a16400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and hyperparameter tuning for Gradient Boosting...\n",
      "Best Gradient Boosting model: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Cross-validation after tuning for Gradient Boosting:\n",
      "CV Mean RMSE (after tuning): 0.25015986428846315, CV RMSE Std: 0.03405469028426656\n",
      "CV Mean R² (after tuning): 0.8956431106552933, CV R² Std: 0.012372533749280636\n",
      "\n",
      "Test RMSE: 0.5361511980236687\n",
      "Test R²: 0.8870845114157249\n",
      "Test CV Mean RMSE: 0.668335105072102, Test CV RMSE Std: 0.16382534477374877\n",
      "Test CV Mean R²: 0.7338897462147209, Test CV R² Std: 0.056960383952345595\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel(r\"C:/Users/dhanu/OneDrive/Desktop/machine learning/ML TRAIN DATASETS/gpt2 embeddings.xlsx\")\n",
    "\n",
    "# Drop any irrelevant columns, such as text or index columns\n",
    "data = data.drop(columns=['Equation', 'GPT2_Embedding'], errors='ignore')\n",
    "\n",
    "# Features and target variable\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "y = data['output']     # Target variable\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.95)  # Preserve 95% of the variance\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Gradient Boosting model\n",
    "model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],           # Number of boosting rounds\n",
    "    'learning_rate': [0.05, 0.1],         # Step size shrinkage\n",
    "    'max_depth': [3, 5],                  # Maximum depth of a tree\n",
    "    'subsample': [0.8, 1.0]               # Fraction of samples used for fitting the trees\n",
    "}\n",
    "\n",
    "print(f\"Training and hyperparameter tuning for Gradient Boosting...\")\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)  # 5-fold CV\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best Gradient Boosting model: {grid_search.best_params_}\")\n",
    "\n",
    "# Function to calculate and return performance metrics\n",
    "def evaluate_model(model, X, y):\n",
    "    cv_scores_rmse = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_scores_r2 = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "\n",
    "    rmse_mean = -cv_scores_rmse.mean()   # Convert negative RMSE to positive\n",
    "    rmse_std = cv_scores_rmse.std()\n",
    "    r2_mean = cv_scores_r2.mean()\n",
    "    r2_std = cv_scores_r2.std()\n",
    "\n",
    "    return rmse_mean, rmse_std, r2_mean, r2_std\n",
    "\n",
    "# Cross-validation performance\n",
    "rmse_mean, rmse_std, r2_mean, r2_std = evaluate_model(best_model, X_train, y_train)\n",
    "\n",
    "print(f\"Cross-validation after tuning for Gradient Boosting:\")\n",
    "print(f\"CV Mean RMSE (after tuning): {rmse_mean}, CV RMSE Std: {rmse_std}\")\n",
    "print(f\"CV Mean R² (after tuning): {r2_mean}, CV R² Std: {r2_std}\\n\")\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Cross-validation performance on the test set\n",
    "test_cv_rmse_mean, test_cv_rmse_std, test_cv_r2_mean, test_cv_r2_std = evaluate_model(best_model, X_test, y_test)\n",
    "\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test R²: {test_r2}\")\n",
    "print(f\"Test CV Mean RMSE: {test_cv_rmse_mean}, Test CV RMSE Std: {test_cv_rmse_std}\")\n",
    "print(f\"Test CV Mean R²: {test_cv_r2_mean}, Test CV R² Std: {test_cv_r2_std}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0003b9b6-65f6-4fd3-a624-10e6a21d42c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and hyperparameter tuning for Random Forest...\n",
      "Best Random Forest model: {'bootstrap': True, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Cross-validation after tuning for Random Forest:\n",
      "CV Mean RMSE (after tuning): 0.33486117360006595, CV RMSE Std: 0.037105468809639934\n",
      "CV Mean R² (after tuning): 0.8602086618624869, CV R² Std: 0.013490170627949931\n",
      "\n",
      "Test RMSE: 0.5622013612642355\n",
      "Test R²: 0.8758454208199241\n",
      "Test CV Mean RMSE: 0.8086493971518987, Test CV RMSE Std: 0.16689611682845654\n",
      "Test CV Mean R²: 0.6784404121509457, Test CV R² Std: 0.05392955352349438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel(r\"C:/Users/dhanu/OneDrive/Desktop/machine learning/ML TRAIN DATASETS/gpt2 embeddings.xlsx\")\n",
    "\n",
    "# Drop any irrelevant columns, such as text or index columns\n",
    "data = data.drop(columns=['Equation', 'GPT2_Embedding'], errors='ignore')\n",
    "\n",
    "# Features and target variable\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "y = data['output']     # Target variable\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.95)  # Preserve 95% of the variance\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Random Forest model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],           # Number of trees\n",
    "    'max_depth': [None, 10, 20],          # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5],          # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2],           # Minimum number of samples required to be at a leaf node\n",
    "    'bootstrap': [True, False]            # Whether bootstrap samples are used when building trees\n",
    "}\n",
    "\n",
    "print(f\"Training and hyperparameter tuning for Random Forest...\")\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)  # 5-fold CV\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best Random Forest model: {grid_search.best_params_}\")\n",
    "\n",
    "# Function to calculate and return performance metrics\n",
    "def evaluate_model(model, X, y):\n",
    "    cv_scores_rmse = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_scores_r2 = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "\n",
    "    rmse_mean = -cv_scores_rmse.mean()   # Convert negative RMSE to positive\n",
    "    rmse_std = cv_scores_rmse.std()\n",
    "    r2_mean = cv_scores_r2.mean()\n",
    "    r2_std = cv_scores_r2.std()\n",
    "\n",
    "    return rmse_mean, rmse_std, r2_mean, r2_std\n",
    "\n",
    "# Cross-validation performance\n",
    "rmse_mean, rmse_std, r2_mean, r2_std = evaluate_model(best_model, X_train, y_train)\n",
    "\n",
    "print(f\"Cross-validation after tuning for Random Forest:\")\n",
    "print(f\"CV Mean RMSE (after tuning): {rmse_mean}, CV RMSE Std: {rmse_std}\")\n",
    "print(f\"CV Mean R² (after tuning): {r2_mean}, CV R² Std: {r2_std}\\n\")\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Cross-validation performance on the test set\n",
    "test_cv_rmse_mean, test_cv_rmse_std, test_cv_r2_mean, test_cv_r2_std = evaluate_model(best_model, X_test, y_test)\n",
    "\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test R²: {test_r2}\")\n",
    "print(f\"Test CV Mean RMSE: {test_cv_rmse_mean}, Test CV RMSE Std: {test_cv_rmse_std}\")\n",
    "print(f\"Test CV Mean R²: {test_cv_r2_mean}, Test CV R² Std: {test_cv_r2_std}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "137fb012-f8d1-4fc0-b601-a80329f42979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and hyperparameter tuning for SVR...\n",
      "Best SVR model: {'C': 10, 'epsilon': 0.1, 'kernel': 'rbf'}\n",
      "Cross-validation after tuning for SVR:\n",
      "CV Mean RMSE (after tuning): 0.1898483429089733, CV RMSE Std: 0.021740942479126044\n",
      "CV Mean R² (after tuning): 0.9207026233798714, CV R² Std: 0.008302080046288355\n",
      "\n",
      "Test RMSE: 0.43865476817161697\n",
      "Test R²: 0.9244169060486128\n",
      "Test CV Mean RMSE: 0.4870366854198326, Test CV RMSE Std: 0.13866470743768772\n",
      "Test CV Mean R²: 0.8040143245511251, Test CV R² Std: 0.05617175036449698\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel(r\"C:/Users/dhanu/OneDrive/Desktop/machine learning/ML TRAIN DATASETS/gpt2 embeddings.xlsx\")\n",
    "\n",
    "# Drop any irrelevant columns, such as text or index columns==\n",
    "data = data.drop(columns=['Equation', 'GPT2_Embedding'], errors='ignore')\n",
    "\n",
    "# Features and target variable\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "y = data['output']     # Target variable\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.95)  # Preserve 95% of the variance\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# SVR model\n",
    "model = SVR()\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],           # Regularization parameter\n",
    "    'epsilon': [0.1, 0.2, 0.5],  # Epsilon in the epsilon-SVR model\n",
    "    'kernel': ['linear', 'rbf']  # Kernel type to be used\n",
    "}\n",
    "\n",
    "print(f\"Training and hyperparameter tuning for SVR...\")\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)  # 5-fold CV\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best SVR model: {grid_search.best_params_}\")\n",
    "\n",
    "# Function to calculate and return performance metrics\n",
    "def evaluate_model(model, X, y):\n",
    "    cv_scores_rmse = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_scores_r2 = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "\n",
    "    rmse_mean = -cv_scores_rmse.mean()   # Convert negative RMSE to positive\n",
    "    rmse_std = cv_scores_rmse.std()\n",
    "    r2_mean = cv_scores_r2.mean()\n",
    "    r2_std = cv_scores_r2.std()\n",
    "\n",
    "    return rmse_mean, rmse_std, r2_mean, r2_std\n",
    "\n",
    "# Cross-validation performance\n",
    "rmse_mean, rmse_std, r2_mean, r2_std = evaluate_model(best_model, X_train, y_train)\n",
    "\n",
    "print(f\"Cross-validation after tuning for SVR:\")\n",
    "print(f\"CV Mean RMSE (after tuning): {rmse_mean}, CV RMSE Std: {rmse_std}\")\n",
    "print(f\"CV Mean R² (after tuning): {r2_mean}, CV R² Std: {r2_std}\\n\")\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Cross-validation performance on the test set \n",
    "test_cv_rmse_mean, test_cv_rmse_std, test_cv_r2_mean, test_cv_r2_std = evaluate_model(best_model, X_test, y_test)\n",
    "\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test R²: {test_r2}\")\n",
    "print(f\"Test CV Mean RMSE: {test_cv_rmse_mean}, Test CV RMSE Std: {test_cv_rmse_std}\")\n",
    "print(f\"Test CV Mean R²: {test_cv_r2_mean}, Test CV R² Std: {test_cv_r2_std}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f87cb58-a4d2-4dfd-a52d-a7747de35a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
